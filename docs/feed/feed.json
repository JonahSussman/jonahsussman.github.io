{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Jonah Sussman",
  "language": "en",
  "home_page_url": "https://jonahsussman.net/",
  "feed_url": "https://example.com/feed/feed.json",
  "description": "",
  "author": {
    "name": "Your Name Here",
    "url": "https://example.com/about-me/"
  },
  "items": [{
      "id": "https://jonahsussman.net/posts/2022-01-this-wiki-dne/",
      "url": "https://jonahsussman.net/posts/2022-01-this-wiki-dne/",
      "title": "This Wikipedia Article Doesn&#39;t Exist",
      "content_html": "<p><a style=\"float: right;\" href=\"https://jonahsussman.net/projects/this-wiki-dne/\"><img src=\"https://jonahsussman.net/assets/projects/wiki-thumb.png\" /></a></p>\n<p>Have you ever wondered how an AI would interpret the world? How it would see the universe? How it would parse knowledge? How it would dream? <a href=\"https://jonahsussman.net/projects/this-wiki-dne\">Meet Tensorpedia! - The latest project I've created</a>. It answers... none of the previous questions, but I think what I've created is interesting nonetheless.</p>\n<p style=\"text-align: center;\"><a href=\"https://jonahsussman.net/projects/this-wiki-dne\">Click here to check it out!</a></p>\n<h2 id=\"background\">Background <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#background\">#</a></h2>\n<blockquote>\n<p>An infinite number of monkeys something something Shakespeare. Yeah, that sounds about right.<br />\n<cite>Ã‰mile Borel, 1913</cite></p>\n</blockquote>\n<p>It's no secret that the Artificial Intelligence and Machine Learning fields have been exploding in recent years. Sure, they can trace their roots all the way back to <a href=\"https://www.dataversity.net/a-brief-history-of-machine-learning/\">the 1950's</a>. However, we're in the midst of a gigantic <a href=\"https://www.researchgate.net/figure/Evolution-on-the-number-of-papers-published-on-DL-topics-with-respect-to-those-on-DL-in_fig1_323444210\">exponential growth in the field</a>. It's a combination of people realizing the potential that these models can have and the absolute raw parallel computing power that we have available nowadays. In any case, I think the whole thing is just neat.</p>\n<p>A while back on the Internet, there was an explosion of AI-generated content. We finally reached a point where the average Joe can make a request to a neural network and have it generate something that &quot;doesn't exist&quot; on-demand. From <a href=\"https://15.ai/\">making your favorite characters say things</a>, to generating <a href=\"https://thispersondoesnotexist.com/\">realistic looking people</a>, and having an AI <a href=\"https://app.inferkit.com/demo\">finish your sentences</a>, I was completely enamored with the stuff. Though, I never thought that I had the skill set in order to make anything as high quality as the aforementioned sites... Until the beginning of this December.</p>\n<p>I was looking for something to do between my fall 2021 and as-of-yet-not-begun spring 2022 semesters. I came up with an idea for another project (that I won't divest too much information on) that involved using the Wikipedia API. I later came across Wikipedia's <a href=\"https://en.wikipedia.org/wiki/Wikipedia:Vital_articles/Level/4\">vital articles lists</a>, articles which Wikipedia believes talk about the most important topics in their English Encyclopedia. Furthermore, I realized that I could fairly easily scrape the titles of these articles via their incredible <a href=\"https://petscan.wmflabs.org/\">petscan tool</a>.</p>\n<p>It's like the idea for Tensorpedia was staring me straight in the face. I had input in the form of Wikipedia titles, output in the form of Wikipedia articles, and could get a whole lot of both from the API. This task was ripe for an AI-based project. And so, I set to work creating Tensorpedia.</p>\n<h2 id=\"my-web-design-philosophy\">My Web Design Philosophy <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#my-web-design-philosophy\">#</a></h2>\n<figure>\n<video width=\"256\" height=\"256\" autoplay=\"\" muted=\"\" loop=\"\">\n  <source src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/explode-256.webm\" type=\"video/webm\" />\n  Your browser does not support the video tag. Imagine there was a video here\n</video> \n<figcaption>\nArtist's interpretation of the Wikimedia foundation when they find out what I've been doing with their data\n</figcaption>\n</figure>\n<p>My philosophy in web design (heavily inspired by the creator of <a href=\"https://svelte.dev/\">Svelte</a>, <a href=\"https://twitter.com/Rich_Harris?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor\">Rich Harris</a>) is that most websites nowadays really don't need big fancy frameworks. While things like React, Vue and Angular are all well and good, all most websites need are sprinkles of reactivity interspersed within mostly static content. For instance, this website you're looking at is hosted entirely on <a href=\"https://github.com/JonahSussman/jonahsussman.github.io\">GitHub pages</a> built using a nifty tool called <a href=\"https://www.11ty.dev/\">Eleventy</a>.</p>\n<p>This philosophy of reducing the amount of JavaScript bloat within the sites I design has three major benefits: The first and more altruistic result is that my website is <small>reasonably</small> fast and light. If there's nothing to weigh it down, there's nothing to weigh it down. It's a tautology, sure, but the truth. The second benefit is that I don't have to worry about web security this way. If my GitHub Pages site gets hacked, then that's GitHub's problem. And something tells me that GitHub dedicates many more man-hours making sure they don't get hacked than I ever could to my site. The final, and pretty selfish reason, is that it's plain cheaper to host a static site. I'm <em>definitely</em> not made of money, and hosting a custom server requires some of it.</p>\n<p>All of this was the motivation behind my first (read: failed) idea.</p>\n<p>One way to generate text when it comes to neural networks is to use what's called a <a href=\"https://en.wikipedia.org/wiki/Recurrent_neural_network\">recurrent neural network</a>. Without getting too much into the weeds, it's a pretty powerful type of neural network. Most importantly to me though is that the neural networks are small.</p>\n<p>In fact, according to the library I was using, <a href=\"https://github.com/minimaxir/textgenrnn\">textgenrnn</a>, the model weighs in at around only 2 MB. Great! I can send over the whole program in a website, and have everything computed on the client-side! No extra server costs needed! I left my computer overnight to churn away at the data I had gathered.</p>\n<p>The next day, I excitedly thought of what I could generate an article about. Toasters, how about toasters! This is what I saw a minute later after asking it about this prompt:</p>\n<blockquote>\n<p><code><b>Toaster</b>: Bank is also called nationalism and despression of the equation and the Achaemenid Empire. The Allied independent in the world. Protective also produces a social science of a process that continue to distinguogy from the professional law of natural and polar is the stone is a member of the primary science of the world's sexual or international processes of the present day...<br />\n</code></p>\n</blockquote>\n<p>Never mind the fact that &quot;despression&quot; and &quot;distinguogy&quot; aren't real words; it said nothing about toasters or anything remotely toaster-adjacent! Unfortunately, LSTMs don't have that much power when it comes to generating coherent pieces of text on a specific topic. This idea was a bust. However, I did know of another type of neural network that generates text on-topic fairly well.</p>\n<h2 id=\"transformer-neural-networks\">Transformer Neural Networks <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#transformer-neural-networks\">#</a></h2>\n<p><a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\">Transformers</a> are another type of neural network that can be used to generate text. They're even more complicated than the basic LSTMs I was trying to use, but the results that they generate are incredible. I first became aware of them when <a href=\"https://openai.com/blog/better-language-models/\">GPT-2 came out</a> and flipped the world of AI on its head. OpenAI released pre-trained versions of GPT-2 that anybody can use and fine-tune to their specific need.</p>\n<p>The only problem? The lightest version of GPT-2 has 124 million parameters, expanding to around <strong>five hundred megabytes</strong> unzipped. Imagine asking people to download the equivalent of <a href=\"https://www.wired.com/2016/04/average-webpage-now-size-original-doom/\">200 DOOMs</a> or <a href=\"https://store.steampowered.com/app/70/HalfLife/\">three and a half Half-lives</a>. No Bueno.</p>\n<p>So I finally had to just bite the bullet and finally <a href=\"https://api.jonahsussman.net/\">buy myself a server</a>. No way I was going to host this thing on my property - there are just too many security risks. Instead, staring at my bank account, I wound up purchasing a six-dollar-a-month single-core DigitalOcean Droplet. It's most definitely not the fastest thing in the world. In fact, it doesn't even have a GPU, something that would make AI generation <em>much</em> faster. But it's cheap and has high uptime, and that's all that matters to me right now.</p>\n<p><small>If Tensorpedia gets popular enough, maybe I'll take a look at <a href=\"https://www.linode.com/products/gpu/\">Linode GPU instances</a> or something.</small></p>\n<h2 id=\"the-%22guts%22-of-the-project\">The &quot;Guts&quot; of the Project <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#the-%22guts%22-of-the-project\">#</a></h2>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/library.jpg\" />\n<figcaption>\nIn some sense, Tensorpedia \"learned\" about all ten thousand different vital articles.<br />\nImage found via <a href=\"https://museo.app/?q=library\">Museo</a> and the <a href=\"https://www.rijksmuseum.nl/en/collection/RP-P-2015-26-2027\">Rijksmuseum</a>.\n</figcaption>\n</figure>\n<p>I'll spare you the stressful details of trying to set up HTTPS with the default configuration of Nginx on the server. I'll not mention the confusion that Python versioning conflicts on Ubuntu 20.04 caused me. I'll even keep mum about the fact that the <strong>paid</strong> version of Google Colab kept resetting all my training progress by logging me out <strong>four times</strong>.</p>\n<p>No, I won't mention any of that. Instead, I'll describe what the whole system ended up looking like at the end.</p>\n<p>The front end is quite simple. It's just some CSS, HTML, and vanilla JS. I didn't see the need for Tensorpedia to include a big framework like React or Vue for something that's, at its core, not terribly complicated. The only library I <em>do</em> depend on is <a href=\"https://socket.io/\">Socket.io</a>. When the user opens the page, the browser creates a Websocket connection to the API server. Then, when they &quot;navigate&quot; to a new page, the query gets pushed to the browser's history, and a request is made over the socket.</p>\n<p>The server is also quite simple. It has three main parts: the aitextgen Python program, the <a href=\"https://redis.io/\">Redis cache</a>, and the node.js server. When the server recieves a request over a client's socket, it first checks whether the client wants to regenerate the page or not. If they don't, it checks the Redis cache to see if the page has been generated already. If they want a new page or it's not in the cache, the request is queued over to the python program via UNIX sockets. The result is then stored in the cache, and sent back over to the user.</p>\n<p><small>Note: this does create a bottleneck at the Python program, but as my server is vastly underpowered, it kinda needs to be that way. Tensorflow takes an incredible amount of CPU resources, and processing the requests one at a time alleviates that stress.</small></p>\n<p>The Python program was trained using the scraped Wikipedia API data utilizing the <a href=\"https://github.com/minimaxir/aitextgen\">aitextgen</a> package on Google Colab. I then transferred the trained model to the server. The library provided a significant speedup over just using raw GPT-2 generation tools. Performance quite matters when you only have one core. Hat's off to minimaxir for creating such an easy-to-use tool.</p>\n<p>It all seems simple, and in retrospect it really is. However, the devil is in the details.</p>\n<h2 id=\"examples-of-generated-articles\">Examples of Generated Articles <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#examples-of-generated-articles\">#</a></h2>\n<p>Finally, let's check out some examples of what Tensorpedia can do! If you skipped to this section, I see you. If you're on a mobile device, I'd recommend opening these images in a new tab.</p>\n<p>First up, let's generate an article on Toasters!</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-toaster.jpg\" />\n</figure>\n<p>You can really see the bias of the ten-thousand vital articles come through. The vital articles contain big bombastic events in history, incredibly influential figures, or abstract ideas like love. Toasters just don't fit into those categories. Overall, B+ effort by Tensorpedia.</p>\n<p>However, as a side effect, Tensorpedia will tend to think everyone is a super important person, every place is a historical place, and every object has a world-changing idea behind it. Let's look at the most average guy in history, Joe Shmoe.</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-joe.jpg\" />\n</figure>\n<p>New Zealanders, is Joe Shmoe your best-kept secret to your Olympic swimming success? Please let me know. Additionally, though I can't show just one example, the bias of time periods tends to be almost exclusively during the 19th and 20th centuries. I thought I'd just share that as an interesting note of where our bias of &quot;what's important&quot; lies.</p>\n<p>Repetition is an issue in any transformer neural network. You'll notice that certain phrases are repeated once or twice if you browse Tensorpedia enough. But you know what area Tensorpedia had trouble with the most in this regard? Russian. For example:</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-igor-bad.jpg\" />\n</figure>\n<p>Now I don't speak Russian, but I'm pretty sure that's nonsense. Luckily, aitextgen includes a parameter to discourage repetition. After enabling it, this is what we get:</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-igor-good.jpg\" />\n</figure>\n<p>Just a little bit better, don't you think?</p>\n<p>Next, I want to share what I think is the coolest article I came across while building Tensorpedia. I put in the prompt &quot;Awesomium&quot; and it gave me this:</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-awesome.jpg\" />\n</figure>\n<p>All just from the title &quot;Awesomium,&quot; it deduced that it was probably an element, its atomic symbol, number, etymology, and history. Pretty crazy!</p>\n<p>The final three examples I'll give to you in rapid-fire mode. The first example shows that Tensorpedia can link to &quot;interesting&quot; (yet plausible!) words.</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-unkleothesis.jpg\" />\n</figure>\n<p>The next one shows that Tensorpedia has a rudimentary understanding of history, generating dates from the appropriate period for Rome.</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-rome.jpg\" />\n</figure>\n<p>Finally, even though Tensorpedia may not know about pop culture, it can be scarily accurate about the mathematical objects pop culture references.</p>\n<figure>\n<img src=\"https://jonahsussman.net/assets/posts/2022-01-this-wiki-dne/example-matrix.jpg\" />\n</figure>\n<h2 id=\"future-improvements-and-conclusion\">Future Improvements and Conclusion <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2022-01-this-wiki-dne/#future-improvements-and-conclusion\">#</a></h2>\n<p>All in all, I'm extremely pleased with how this project turned out. I wanted to get this project finished before the new year. Oh well, c'est la vie, I suppose.</p>\n<p>That is not to say, however, I think there's no room for improvement.</p>\n<p>For one, the neural network works, but it's painfully slow sometimes. I've had articles where it takes on the order of 2 minutes to generate. Far too long! There's not much I can do aside from buying more power on the server. One thing I have been looking at is potentially using <a href=\"https://github.com/microsoft/onnxruntime\">Microsoft's ONNX runtime</a> to speed it up, but that would require digging into the guts of aitextgen to do.</p>\n<p>Another improvement is that I'm not quite used to event-driven programming yet. I utilized quite a bit of &quot;async&quot; and &quot;await&quot; in my code, but I'm sure there's a more clever and elegant way to accomplish what I want to do. Perhaps this could allow me to utilize Redis more effectively as well.</p>\n<p>But, as I stated before, I'm quite happy with how Tensorpedia turned out.</p>\n<p>Thank you for reading, and please enjoy Tensorpedia!</p>\n",
      "date_published": "2022-01-02T00:00:00Z"
    },{
      "id": "https://jonahsussman.net/posts/2021-08-productivity/",
      "url": "https://jonahsussman.net/posts/2021-08-productivity/",
      "title": "The Perfect Productivity System Doesn&#39;t Exist",
      "content_html": "<blockquote>\n<p>Dans ses Ã©crits, un sage Italien; Dit que le mieux est l'ennemi du bien.<br />\n(In his writings, a wise Italian; says that the best is the enemy of the good)<br />\n<cite>Voltaire, 1772</cite></p>\n</blockquote>\n<p>I've thought a lot about productivity over the past couple of years - how to be the most efficient with the time that we have. Especially as I grow older, I've realized that there's a lot of merit to the statement &quot;the only truly non-renewable resource is time&quot;.</p>\n<p>Earlier in life, I didn't really see a need to manage my time. Things just sort of... happened. A project popped up in class, homework came due, or an extracurricular had a meeting. I would keep it all in my head. Maybe I would check the calendar once or twice for a refresher, but it would have to be a pretty important date for me to write it down.</p>\n<p>It was by no means perfect. I would forget due dates. I would spend too long on one thing and then realize that I had no time left for the other. Other times I would procrastinate so long that the items just left my head, and it wasn't until a really inopportune moment that I would remember it. I always felt like I was reacting, never getting out in front of what I had to do.</p>\n<p>Unfortunately, this broken system worked. And it worked well enough for long enough that I got by.</p>\n<p>Then I went off to college.</p>\n<p>I know that it's a clichÃ© at this point to say that college is a big shift in your life. Usually, it really is. You're in a new environment with a bunch of people that you've never met before. You're not going to school, you're taking classes you chose. You're not getting taught by teachers, but by professors. Oh yeah, I almost forgot to mention the most important one!</p>\n<p>You're not finishing your first year there, you're going back home due to the plague.</p>\n<p>The initial news of COVID-19 was a big shift for everybody. What I realized, however, was that newly off to college mixed with a dash of a global pandemic, folded in with my non-existent productivity system was, you guessed it, a recipe for disaster.</p>\n<h2 id=\"friction\">Friction <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2021-08-productivity/#friction\">#</a></h2>\n<p><a href=\"https://www.youtube.com/watch?v=k2Wcu6aGyz8\">Willpower is for losers.</a> Or rather, our environment has a profound effect on us. As much as we would like to think that we are machines or disembodied consciousnesses that are unaffected by where they are, we are not. We physically exist in an environment, and that environment can present certain &quot;frictions&quot; to achieving certain goals.</p>\n<p>Being sent home my first year of college shoved this fact straight into my face. Having no method for managing your time is one thing when you absolutely <em>have</em> to be in a certain place at a certain time. The friction to do anything other than focus and learn is incredibly high. It's an entirely other thing when all of your classesâ€™ lectures are pre-recorded and you can watch them at any time. There's practically no friction at all to prevent you from distractions.</p>\n<p>I came to realize a couple of things:</p>\n<ol>\n<li>My &quot;rolling with the punches&quot; system <em>had to go</em>.</li>\n<li>I needed to find or create a robust system that could handle whatever I threw at it</li>\n<li>This system needed to be as frictionless as possible in order to keep me centered through times of stress</li>\n</ol>\n<p>Unfortunately for me, there's a deluge of different productivity apps these days. OneNote, Notion, TiddlyWiki, Zim, CherryTree, org-mode, etc... And it certainly doesn't help that <a href=\"https://news.ycombinator.com/item?id=27537255\">&quot;the frontier between 'note taking' and 'personal project managementâ€™, or 'todo list' is blurry as they're usually done with (the) same tools&quot;</a>. On top of that, there are so many different productivity systems out there that take advantage of these tools. GTD, PARA, Bullet Journal, etc...</p>\n<p>I literally tried them all. Every time something didn't feel quite right, I would jump ship and change systems. Some systems used tagging, but I felt like some things needed more structure. Other times I locked my stuff into a hierarchical structure and missed the freeform nature tags. I wanted <em>all</em> of the features in my system. I thought that the freedom that this would allow me, would allow me to be the most productive version of myself.</p>\n<p>However, this is a well-known problem, that no matter how hard a system tries, after it gets large enough <a href=\"https://en.wikipedia.org/wiki/Everything_Is_Miscellaneous\">everything becomes miscellaneous</a>. Once I realized this, I had an epiphany. If all the systems are garbage in their own way, I might as well have a simple one (though maybe not as simple as a <a href=\"https://jeffhuang.com/productivity_text_file/\">single text file</a>).</p>\n<p>And so what follows is what I've come up with</p>\n<h2 id=\"my-system\">My system <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2021-08-productivity/#my-system\">#</a></h2>\n<p>My system consists of 4 lists, in the same vein as a bullet journal.</p>\n<p>The first list is my <em>inbox.</em> Any time I had an idea, or a task that I wanted to do, I put it in my inbox. This was the biggest help by far. I know it seems trivial, but it really changed my whole world. No longer did I have to worry about forgetting something that I thought about. I could just write it down and come back to it later.</p>\n<p>The next 3 lists are the <em>week, month, and quarter</em> lists. Anything that I want to keep track of for these time periods are transferred from the inbox to these lists when I have the time. (If the thing takes less than 2 minutes, I just go ahead and do it, then remove it from the inbox). Every time I create a new one of these lists (at the beginning of the week/month/quarter) I'll take the unfinished tasks and migrate them over if they're important, or if they're not I'll erase them.</p>\n<p>Notice how I haven't mentioned any tool or system? If you must know, I use Google Keep to store my lists. The biggest reasons were: easy data backup, accessible on my phone, and excellent search tool. But really, this system could be used on pen and paper just as well.</p>\n<p>And that's literally it. If I need to be at a certain place at a certain time, I'll put it on my calendar. If I need to take some notes for a specific project, I'll bust out a separate list for it, but the main tasks stay on these 4 lists.</p>\n<h2 id=\"finishing-up\">Finishing up <a class=\"direct-link\" href=\"https://jonahsussman.net/posts/2021-08-productivity/#finishing-up\">#</a></h2>\n<p>With this ridiculously simple system, I'm finding that I'm forgetting to do things less. I feel like I have control of what I'm doing, and feeling better about the things that I'm not doing. In fact, it's so simple and so helpful that I'm wondering why I didn't start this years ago.</p>\n",
      "date_published": "2021-08-11T00:00:00Z"
    }
  ]
}
